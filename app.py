# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NjCIYfv0WXV76dwqjjSby-jm9Di7y6n_
"""

import streamlit as st
from diffusers import StableDiffusionPipeline
import torch
from gtts import gTTS
from moviepy.video.VideoClip import ImageClip
from moviepy.video.compositing.concatenate import concatenate_videoclips
from moviepy.audio.io.AudioFileClip import AudioFileClip
from PIL import Image
# import tempfile

st.title("Text-to-Video Generator")

scene1 = st.text_input("Scene 1 description", "A futuristic city skyline at sunrise")
scene2 = st.text_input("Scene 2 description", "A serene forest with a crystal-clear river")
scene3 = st.text_input("Scene 3 description", "An epic space battle with spaceships")

if st.button("Generate Video"):
    prompts = [scene1, scene2, scene3]

    st.info("Generating images... this may take some time.")

    # Load diffusion model
    pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    pipe = pipe.to(device)

    images = []
    audio_files = []

    for prompt in prompts:
        image = pipe(prompt, guidance_scale=7.5).images[0]
        images.append(image)

        tts = gTTS(text=prompt, lang="en")
        audio_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3").name
        tts.save(audio_file)
        audio_files.append(audio_file)

    st.success("Images + audio generated. Creating video...")

    scene_clips = []
    for i, image in enumerate(images):
        clip = ImageClip(image).set_duration(5)
        audio = AudioFileClip(audio_files[i])
        clip = clip.set_audio(audio)
        scene_clips.append(clip)

    final_video = concatenate_videoclips(scene_clips)
    video_file = "final_video.mp4"
    final_video.write_videofile(video_file, fps=30)

    st.video(video_file)
    st.success("Video generated successfully!")